# Project Yad â€“ EMG-Based Gesture Recognition

Project Yad is a collaborative EMG-based gesture recognition system built by **Guy Katabi** and **Tomer Ohayon**.  
It uses EMG signals from an armband (e.g., MindRove) and processes them in real-time on a Raspberry Pi to classify hand gestures.

## ðŸ”§ Features

- Real-time EMG acquisition and filtering  
- Feature extraction (e.g., MAV, RMS, WL, ZC, SSC, WAMP, etc.)  
- MLP / SVM-based gesture classification  
- Configurable paths and parameters via `assets/config.json`  
- Designed to run on Raspberry Pi 5

## ðŸ“‚ Project Structure

```text
Project_Yad/
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ models/          # (optional, typically kept local or small demo models)
â”œâ”€â”€ realtime/
â”‚   â”œâ”€â”€ feature_extractor.py
â”‚   â”œâ”€â”€ filters.py
â”‚   â”œâ”€â”€ mlp_inference.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ training_data/       # raw/processed data and experiments (not all tracked in git)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


how to start:

git clone https://github.com/guyk1000-hub/Project_Yad.git
cd Project_Yad

python3 -m venv venvPI
source venvPI/bin/activate   # On Linux/macOS

python -m pip install --upgrade pip
python -m pip install -r requirements.txt


make sure assets/config.json exists and is configured correctly

source venvPI/bin/activate
python main_pi.py

Authors:

Guy Katabi â€“ @guyk1000-hub

Tomer Ohayon â€“ @tomerohayon77

Acknowledgements:
This project was inspired by and partially adapted from:

MindRove / NaviFlame
 â€“ EMG data streaming and control interface

tomerohayon77 / mindrove-emg-classifier
 â€“ feature extraction and classification pipeline

We thank the authors of these open-source projects for their contributions, which helped guide parts of this work.

ðŸ“œ License

This project is licensed under the MIT License (or another license you choose).
